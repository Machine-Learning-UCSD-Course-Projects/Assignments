\documentclass[11pt,a4paper,oneside]{article}
%\documentclass[a4paper]{scrartcl}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={210mm,297mm},
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
 }
 
\usepackage{enumitem}
\usepackage{color}
\def\red{\textcolor{red}}
\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

\usepackage{float}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\title{Topic Modelling using Latent Dirichlet Allocation}
\subtitle{CSE 250B Project 3}
\author{Suvir Jain, Gaurav Saxena}
\date{27 February,2014}

\begin{document}
\maketitle

\begin{abstract}
Abstract about LDA, our data, brief results.
\end{abstract}

\section{Introduction}

The objective of the project is to learn a Latent Dirichlet Allocation (LDA) model to predict dominant topics associated with a collection documents. This process is called topic modelling.

We discuss the LDA framework for topic modelling in section \ref{sec:Framework}. We, then, describe our implementation of algorithm based on the framework in section \ref{sec:Algorithms}. Furthermore, we discuss our experiments and code optimizations in \ref{sec:Experiments} and finally present results in \ref{sec:Results} and lessons learnt in section \ref{Lessons}

We use two data sets classic 400[\red{reference}] and twitter tobacco [\red{reference}] for topic modelling. The first data sets is accompanied with true labels for the topics. We use this data to train the model using Collapsed Gibbs (CGS) Sampling and verify our results.

We use this model to predict the topics for the tobacco data set. We discuss the results and how they relate to real world and our assumptions.

\section{Framework}
\label{sec:Framework}

\subsection{Dataset}

\paragraph{Classic400}

This dataset is a part of classic3\cite{banerjee2005clustering} dataset. Classic3 dataset is a collection of 3893 documents, which in turn contains 1400 CRANFIELD documents from aeronautical systems papers, 1033 MEDLINE documents from medical journals and 1460 CISI documents from information retrieval papers. Classic400 is a collection of 400 documents randomly chosen from classic3 dataset by \cite{banerjee2005clustering}. We use the same dataset for our analysis.

This data set contains a total of 6505 words and hence each document is converted to a vector of 6505 integers, each showing the frequency a word in a document. The dataset also contains true labels of the dataset which we use for sanity check of our model. The dataset also contains the vocabulary used in the classic400. We use the vocabulary to find the top words for each topic.

\paragraph{Second data set}
Add more details for the second dataset and how you chose it.

\subsection{Model}
\subsubsection{Representation}
LDA uses a vector of length equal to the vocabulary to represent each document. Each element is represent the frequency of words found in that document. As discussed in class notes, this model is called bag of words. This representation doesn't preserve the word order and consequently loses information. However, as discussed in class notes, this representation may be sufficient to deduce topics from the documents. Therefore, for a document $d$ and a vector $\bar{x}$ then $x_j$ is the number of word $j$ in $d$ and the length of a document n is

\begin{equation}
	n = \sum^{m}_{j=1}x_j
\end{equation}

\paragraph{Multinomial Distribution over documents}
Now, given a set of documents and their representation as vectors, we need a model to represent them. A model is a probability distribution over the set of documents. We calculate the parameters of this model such that the training documents have a higher probability. We can, then, use this model to find the probability of a test document.

LDA uses multinomial distribution. Mathematical expression for a multinomial distribution is given below

\begin{equation}\label{multinomial}
p(x;\theta) = (\frac{n!}{\prod^m_{j=1}x_j!})(\prod^m_{j=1}\theta^{x_j}_j)
\end{equation}

where x is is a vector of non-negative integers and parameter $\theta$ are parameters of the model. In this expression, $\theta$ can be argued as a probability of a word $j$ while $x_j$ represents its count.

As discussed in the class notes, the first term in the equation \ref{multinomial}, the first term is the number of sentences which results in the same $x$ vector. The second term is the probability of an equivalence class of $\bar{x}$.

Also, given a set of training documents, the maximum likelihood estimate of the $j_{th}$ parameter is

\begin{equation}
\theta_j = \frac{1}{T} \sum_x x_j
\end{equation}

where the sum is over all documents x to the training set and $T = \sum_x{\sum_j{x_j}} $. However, this expression may lead to $0$ probabilities. Therefore, we add a constant to the expression

\subsubsection{Inference Algorithm for Linear Chain CRF}
How do we interpret results - Theta and Phi.
Mention that theory here.

\subsubsection{Training Methods for LDA}

Theory of Gibbs Sampling and Collapsed Gibbs Sampling

\paragraph*{Gibb's Sampling}

\paragraph*{Collapsed Gibbs Sampling}

\section{Design and Analysis of Algorithms}

\label{sec:Algorithms}
Mention complexities of algorithms.

\subsection{LDA}

\subsection{Collapsed Gibb's Sampling}
Cite [Heinrich, 2005]

\section{Design of Experiments}
\label{sec:Experiments}

\subsection{Dataset Preprocessing}
Mention the stopwords used for processing second dataset.
Cite the link : http://cseweb.ucsd.edu/users/elkan/151/classic400.mat

\subsection{Expt 1}

\subsection{Expt 2}

\subsection{Expt 3}
Show that the algorithms are actually the complexity that we expect them to be.

\subsection{Implementation}
Implementation specific notes

Z is document specific. Therefore it will have a different length for different document.

Also each word can have a different topic, therefore there will be a different entry for each word in Z. However, this is not true about q and n as they are counts. They will have K * V and K * M sizes

\subsection{Code Optimization}
Describe how inner loop was made fast.

\subsection{Sanity Checks}
Comparison with true labels. 
Can make a table here.
Summation of n should be equal to q vector should be equal to classic400.

\section{Results of Experiments}
\label{sec:Results}

\subsection{Result 1}
Describe results of topic modelling in both data sets.
-Set of words associated with the dominant topics
-3d graph for both data sets 
	-- line graph (showing triangle for first one)
	-- cluster graph
-Some measure of goodness of fit
-Give some results related to kappa, alpha and beta
-Say something about overfitting (if applicable)

Correlate both data sets' result to real-world knowledge of the data.

\paragraph{More details about dataset 1 = classic 400}
\paragraph{More details about dataset 2 = chosen data set}

\subsection{Accuracy of LDA}
Include the plot of theta values.

\section{Findings and Lessons Learned}
\label{Lessons}

\subsection{Goodness of Fit}
ADDRESS THE FOLLOWING

In the report, try to answer the following questions. The questions are related to each other, and do not have definitive answers.
1. What is a sensible way to define the goodness-of-fit, for the same dataset, of LDA models with different hyperparameters K, ALPHA, and BETA? (Refer to tips in class notes)
2. Given the definition of goodness-of-fit, is it possible to compute it numerically, either exactly or approximately?
3. How can we determine whether an LDA model is overfitting its training data?
For the two datasets with which you do experiments, present and justify good values for K, ALPHA and BETA. You can choose these values informally (you do not need an automated algorithm) but your choices should be sensible and justified.

\bibliographystyle{abbrv}
\bibliography{Report}

\end{document}