\documentclass[11pt,a4paper,oneside]{article}
%\documentclass[a4paper]{scrartcl}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={210mm,297mm},
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
 }
 
\usepackage{enumitem}
\usepackage{color}
\def\red{\textcolor{red}}
\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

\usepackage{float}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\title{Topic Modelling using Latent Dirichlet Allocation}
\subtitle{CSE 250B Project 3}
\author{Suvir Jain, Gaurav Saxena}
\date{27 February,2014}

\begin{document}
\maketitle

\begin{abstract}
Abstract about LDA, our data, brief results.
\end{abstract}

\section{Introduction}

The objective of the project is to learn a Latent Dirichlet Allocation (LDA) model to predict dominant topics associated with a collection documents. This process is called topic modelling.

We discuss the LDA framework for topic modelling in section \ref{sec:Framework}. We, then, describe our implementation of algorithm based on the framework in section \ref{sec:Algorithms}. Furthermore, we discuss our experiments and code optimizations in \ref{sec:Experiments} and finally present results in \ref{sec:Results} and lessons learnt in section \ref{Lessons}

We use two data sets classic 400[\red{reference}] and twitter tobacco [\red{reference}] for topic modelling. The first data sets is accompanied with true labels for the topics. We use this data to train the model using Collapsed Gibbs (CGS) Sampling and verify our results.

We use this model to predict the topics for the tobacco data set. We discuss the results and how they relate to real world and our assumptions.

\section{Framework}
\label{sec:Framework}

\subsection{Dataset}

\paragraph{Classic400}

This dataset is a part of classic3\cite{banerjee2005clustering} dataset. Classic3 dataset is a collection of 3893 documents, which in turn contains 1400 CRANFIELD documents from aeronautical systems papers, 1033 MEDLINE documents from medical journals and 1460 CISI documents from information retrieval papers. Classic400 is a collection of 400 documents randomly chosen from classic3 dataset by \cite{banerjee2005clustering}. We use the same dataset for our analysis.

This data set contains a total of 6505 words and hence each document is converted to a vector of 6505 integers, each showing the frequency a word in a document. The dataset also contains true labels of the dataset which we use for sanity check of our model. The dataset also contains the vocabulary used in the classic400. We use the vocabulary to find the top words for each topic.

\paragraph{Second data set}
Add more details for the second dataset and how you chose it.

\subsection{Model}
\paragraph{Representation}
LDA uses a vector of length equal to the vocabulary to represent each document. Each element is represent the frequency of words found in that document. As discussed in class notes, this model is called bag of words. This representation doesn't preserve the word order and consequently loses information. However, as discussed in class notes, this representation may be sufficient to deduce topics from the documents. Therefore, for a document $d$ and a vector $\bar{x}$ then $x_j$ is the number of word $j$ in $d$ and the length of a document n is

\begin{equation}
	n = \sum^{m}_{j=1}x_j
\end{equation}

\paragraph{Multinomial Distribution over documents}
Now, given a set of documents and their representation as vectors, we need a model to represent them. A model is a probability distribution over the set of documents. We calculate the parameters of this model such that the training documents have a higher probability. We can, then, use this model to find the probability of a test document.

LDA uses multinomial distribution. Mathematical expression for a multinomial distribution is given below

\begin{equation}\label{multinomial}
p(x;\theta) = (\frac{n!}{\prod^m_{j=1}x_j!})(\prod^m_{j=1}\theta^{x_j}_j)
\end{equation}

where x is is a vector of non-negative integers and parameter $\theta$ are parameters of the model. In this expression, $\theta$ can be argued as a probability of a word $j$ while $x_j$ represents its count.

As discussed in the class notes, the first term in the equation \ref{multinomial}, the first term is the number of sentences which results in the same $x$ vector. The second term is the probability of an equivalence class of $\bar{x}$.

Also, given a set of training documents, the maximum likelihood estimate of the $j_{th}$ parameter is

\begin{equation}
\theta_j = \frac{1}{T} \sum_x x_j
\end{equation}

where the sum is over all documents x to the training set and $T = \sum_x{\sum_j{x_j}} $. However, this expression may lead to $0$ probabilities. Therefore, we add a constant to the expression called a pseudocount. It is a notional number of occurences of each word in each document. The formula, thus, becomes,

\begin{equation}
\theta_j = \frac{1}{T'} (c + \sum_x x_j)
\end{equation}

where $T' = mc + T$ and $0 < c \leq 1 $
\paragraph{Generative Process}
As discussed in the notes, the generative process assumes that the data points are generated by a probabilistic process. It helps to find the parameters of this process by based on the concept of maximum likelihood. The algorithm for the generative process is given in the notes\cite{classNotes} and not reproduced here for brevity. The generative process gives the following global probability distribution which represents a mixture distribution

\begin{equation}
f(x) = \sum^K_{k=1}f(x;\phi_k)
\end{equation}

where $x$ is a document, $\phi_k$ is a parameter value of the $k_{th}$ multinomial and $\alpha_k$ is the proportion of component number k.

In LDA we use Dirichlet Distribution which acts as a prior for multinomial distribution discussed above. It takes the following form \cite{classNotes}

\begin{equation}
p(\gamma|\alpha) = \frac{1}{D(\alpha)}\prod^m_{s=1}\gamma^{\alpha_s - 1}_s
\end{equation}

where ${D(\alpha)} = \int_\gamma \prod^m_{s=1}\gamma^{\alpha_s - 1}_s$

\subsubsection{Inference from LDA}
How do we interpret results - Theta and Phi.
Mention that theory here.

\subsubsection{Training Methods for LDA}

We use the words of the documents as our training data. We assume that the number of topics K, $\alpha$ and $\beta$ are fixed. Our aim is to learn $\theta$ values for documents and $\phi$ values topics. We use collapsed Gibbs Sampling for this process.

\paragraph*{Gibb's Sampling}

\paragraph*{Collapsed Gibbs Sampling}

As discussed in the class notes \cite{classNotes}, we use a variant of Gibbs Sampling called collapsed Gibbs Sampling (CGS) for learning the topics from the documents. In this process we do not try to learn $\theta$ and $\phi$ directly, but try to learn the hidden topic label $z$ for each word in each document.

We represent each word by its position in the document vector and its count as the value of the element at the position. However, CGS is based on the occurrence of the words. Therefore, the length of the z is taken as the sum of all the elements of the vector (or the length of the document).

Gibbs Sampling assumes that we know the $z$ value for an occurrence of word $i$. It, then, draws a random value of topic for every word except $i$ according to a distribution. We use the following distribution for Gibb's Sampling

\begin{equation}
p(z_i|\bar{z}', \bar{w}) = \frac{p(\bar{w} | \bar{z}) p(\bar{z})}{p(w_i|\bar{z}')p(\bar{w}'|\bar{z}')p(\bar{z}')}
\end{equation}

where $\bar{w}$ is the sequence of words of the vocabulary, $\bar{z}$ is the corresponding sequence of z values, $\bar{w}'$ represents $\bar{w}$ without the word $w_i$ and similarly, $\bar{z}'$ represents $\bar{z}$ without the word $w_i$. This expression can be reduced in terms of the counts of topics as given below

\begin{equation}
p(z_i = j | \bar{z}', \bar{w}) = \frac{q_{j w_{i}'} + \beta_{w_i}}{\sum_t{q_{jt}' + \beta_t}}\frac{n_{mj'} + \alpha_j}{\sum_k{n_{mk}' + \alpha_k}}
\end{equation}

where $q_{j w_i}$ is the count of word $w_i$ occurs with topic $j$, $q_{j w_i}'$ is the count of word $w_i$ occurs with topic $j$ except for the word $w_i$, $n_{mk}$ is the count of the number of times topic $z_i = k$ in the document $m$ and $n_{mk}'$ is the count of the number of times topic $z_i = k$ in the document $m$ except for the word $w_i$ 

\section{Design and Analysis of Algorithms}

\label{sec:Algorithms}
Mention complexities of algorithms.

\subsection{LDA}

\subsection{Collapsed Gibb's Sampling}
Cite [Heinrich, 2005]

\section{Design of Experiments}
\label{sec:Experiments}

\subsection{Dataset Preprocessing}
Mention the stopwords used for processing second dataset.
Cite the link : http://cseweb.ucsd.edu/users/elkan/151/classic400.mat

\subsection{Expt 1}

\subsection{Expt 2}

\subsection{Expt 3}
Show that the algorithms are actually the complexity that we expect them to be.

\subsection{Implementation}
Implementation specific notes

Z is document specific. Therefore it will have a different length for different document.

Also each word can have a different topic, therefore there will be a different entry for each word in Z. However, this is not true about q and n as they are counts. They will have K * V and K * M sizes

\subsection{Code Optimization}
Describe how inner loop was made fast.

\subsection{Sanity Checks}
Comparison with true labels. 
Can make a table here.
Summation of n should be equal to q vector should be equal to classic400.

\section{Results of Experiments}
\label{sec:Results}

\subsection{Result 1}
Describe results of topic modelling in both data sets.
-Set of words associated with the dominant topics
-3d graph for both data sets 
	-- line graph (showing triangle for first one)
	-- cluster graph
-Some measure of goodness of fit
-Give some results related to kappa, alpha and beta
-Say something about overfitting (if applicable)

Correlate both data sets' result to real-world knowledge of the data.

\paragraph{More details about dataset 1 = classic 400}
\paragraph{More details about dataset 2 = chosen data set}

\subsection{Accuracy of LDA}
Include the plot of theta values.

\section{Findings and Lessons Learned}
\label{Lessons}

\subsection{Goodness of Fit}
ADDRESS THE FOLLOWING

In the report, try to answer the following questions. The questions are related to each other, and do not have definitive answers.
1. What is a sensible way to define the goodness-of-fit, for the same dataset, of LDA models with different hyperparameters K, ALPHA, and BETA? (Refer to tips in class notes)
2. Given the definition of goodness-of-fit, is it possible to compute it numerically, either exactly or approximately?
3. How can we determine whether an LDA model is overfitting its training data?
For the two datasets with which you do experiments, present and justify good values for K, ALPHA and BETA. You can choose these values informally (you do not need an automated algorithm) but your choices should be sensible and justified.

\bibliographystyle{abbrv}
\bibliography{Report}

\end{document}