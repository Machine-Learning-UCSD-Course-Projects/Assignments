\documentclass[11pt,a4paper,oneside]{article}
%\documentclass[a4paper]{scrartcl}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={210mm,297mm},
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
 }
 
\usepackage{enumitem}
\usepackage{color}
\def\red{\textcolor{red}}
\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

\usepackage{float}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\title{Topic Modelling using Latent Dirichlet Allocation}
\subtitle{CSE 250B Project 3}
\author{Suvir Jain, Gaurav Saxena}
\date{27 February,2014}

\begin{document}
\maketitle

\begin{abstract}
Abstract about LDA, our data, brief results.
\end{abstract}

\section{Introduction}

The objective of the project is to learn a Latent Dirichlet Allocation (LDA) model to predict dominant topics associated with a collection documents. This process is called topic modelling.

We discuss the LDA framework for topic modelling in section \ref{sec:Framework}. We, then, describe our implementation of algorithm based on the framework in section \ref{sec:Algorithms}. Furthermore, we discuss our experiments and code optimizations in \ref{sec:Experiments} and finally present results in \ref{sec:Results} and lessons learnt in section \ref{Lessons}

We use two data sets classic 400[\red{reference}] and twitter tobacco [\red{reference}] for topic modelling. The first data sets is accompanied with true labels for the topics. We use this data to train the model using Collapsed Gibbs (CGS) Sampling and verify our results.

We use this model to predict the topics for the tobacco data set. We discuss the results and how they relate to real world and our assumptions.

\section{Framework}
\label{sec:Framework}

\subsection{Dataset}

\paragraph{Classic400}

This dataset is a part of classic3\cite{banerjee2005clustering} dataset. Classic3 dataset is a collection of 3893 documents, which in turn contains 1400 CRANFIELD documents from aeronautical systems papers, 1033 MEDLINE documents from medical journals and 1460 CISI documents from information retrieval papers. Classic400 is a collection of 400 documents randomly chosen from classic3 dataset by \cite{banerjee2005clustering}. We use the same dataset for our analysis.

This data set contains a total of 6505 words and hence each document is converted to a vector of 6505 integers, each showing the frequency a word in a document. The dataset also contains true labels of the dataset which we use for sanity check of our model. The dataset also contains the vocabulary used in the classic400. We use the vocabulary to find the top words for each topic.

\paragraph{Hookah data set}
This data set consists of a set of tweets about Hookah usage. These tweets are drawn from a larger collection of tweets. The main set of tweets was collected between November 2011 and August 2013 using the Twitter Application Programming Interface (API). These tweets represented 1\% of all tweets on Twitter during this time interval. On average, this data set consisted of 1.3 million tweets per day. Hookah-related tweets were extracted from this set using the keywords: hookah,hooka,waterpipe,shisha,sheesha and water pipe.The hookah data set consisted of 95,738 tweets. 

To allow for experiments to be done in a short amount of time, we could not use all 95,738 tweets. Hence, we extracted 3000 random tweets from this data set and used that for this project. This data set has a vocabulary size of 8021 words.

\subsection{Model}
\paragraph{Representation}
LDA uses a vector of length equal to the vocabulary to represent each document. Each element is represent the frequency of words found in that document. As discussed in class notes, this model is called bag of words. This representation doesn't preserve the word order and consequently loses information. However, as discussed in class notes, this representation may be sufficient to deduce topics from the documents. Therefore, for a document $d$ and a vector $\bar{x}$ then $x_j$ is the number of word $j$ in $d$ and the length of a document n is

\begin{equation}
	n = \sum^{m}_{j=1}x_j
\end{equation}

\paragraph{Multinomial Distribution over documents}
Now, given a set of documents and their representation as vectors, we need a model to represent them. A model is a probability distribution over the set of documents. We calculate the parameters of this model such that the training documents have a higher probability. We can, then, use this model to find the probability of a test document.

LDA uses multinomial distribution. Mathematical expression for a multinomial distribution is given below

\begin{equation}\label{multinomial}
p(x;\theta) = (\frac{n!}{\prod^m_{j=1}x_j!})(\prod^m_{j=1}\theta^{x_j}_j)
\end{equation}

where x is is a vector of non-negative integers and parameter $\theta$ are parameters of the model. In this expression, $\theta$ can be argued as a probability of a word $j$ while $x_j$ represents its count.

As discussed in the class notes, the first term in the equation \ref{multinomial}, the first term is the number of sentences which results in the same $x$ vector. The second term is the probability of an equivalence class of $\bar{x}$.

Also, given a set of training documents, the maximum likelihood estimate of the $j_{th}$ parameter is

\begin{equation}
\theta_j = \frac{1}{T} \sum_x x_j
\end{equation}

where the sum is over all documents x to the training set and $T = \sum_x{\sum_j{x_j}} $. However, this expression may lead to $0$ probabilities. Therefore, we add a constant to the expression called a pseudocount. It is a notional number of occurences of each word in each document. The formula, thus, becomes,

\begin{equation}
\theta_j = \frac{1}{T'} (c + \sum_x x_j)
\end{equation}

where $T' = mc + T$ and $0 < c \leq 1 $
\paragraph{Generative Process}
As discussed in the notes, the generative process assumes that the data points are generated by a probabilistic process. It helps to find the parameters of this process by based on the concept of maximum likelihood. The algorithm for the generative process is given in the notes\cite{classNotes} and not reproduced here for brevity. The generative process gives the following global probability distribution which represents a mixture distribution

\begin{equation}
f(x) = \sum^K_{k=1}f(x;\phi_k)
\end{equation}

where $x$ is a document, $\phi_k$ is a parameter value of the $k_{th}$ multinomial and $\alpha_k$ is the proportion of component number k.

In LDA we use Dirichlet Distribution which acts as a prior for multinomial distribution discussed above. It takes the following form \cite{classNotes}

\begin{equation}
p(\gamma|\alpha) = \frac{1}{D(\alpha)}\prod^m_{s=1}\gamma^{\alpha_s - 1}_s
\end{equation}

where ${D(\alpha)} = \int_\gamma \prod^m_{s=1}\gamma^{\alpha_s - 1}_s$

\subsubsection{Inference from LDA}
How do we interpret results - Theta and Phi.
Mention that theory here.

\subsubsection{Training Methods for LDA}

We use the words of the documents as our training data. We assume that the number of topics K, $\alpha$ and $\beta$ are fixed. Our aim is to learn $\theta$ values for documents and $\phi$ values topics. We use collapsed Gibbs Sampling for this process.

\paragraph*{Gibb's Sampling}

\paragraph*{Collapsed Gibbs Sampling}

As discussed in the class notes \cite{classNotes}, we use a variant of Gibbs Sampling called collapsed Gibbs Sampling (CGS) for learning the topics from the documents. In this process we do not try to learn $\theta$ and $\phi$ directly, but try to learn the hidden topic label $z$ for each word in each document.

We represent each word by its position in the document vector and its count as the value of the element at the position. However, CGS is based on the occurrence of the words. Therefore, the length of the z is taken as the sum of all the elements of the vector (or the length of the document).

Gibbs Sampling assumes that we know the $z$ value for an occurrence of word $i$. It, then, draws a random value of topic for every word except $i$ according to a distribution. We use the following distribution for Gibb's Sampling

\begin{equation}
p(z_i|\bar{z}', \bar{w}) = \frac{p(\bar{w} | \bar{z}) p(\bar{z})}{p(w_i|\bar{z}')p(\bar{w}'|\bar{z}')p(\bar{z}')}
\end{equation}

where $\bar{w}$ is the sequence of words of the vocabulary, $\bar{z}$ is the corresponding sequence of z values, $\bar{w}'$ represents $\bar{w}$ without the word $w_i$ and similarly, $\bar{z}'$ represents $\bar{z}$ without the word $w_i$. This expression can be reduced in terms of the counts of topics as given below

\begin{equation}
p(z_i = j | \bar{z}', \bar{w}) = \frac{q_{j w_{i}'} + \beta_{w_i}}{\sum_t{q_{jt}' + \beta_t}}\frac{n_{mj'} + \alpha_j}{\sum_k{n_{mk}' + \alpha_k}}
\end{equation}

where $q_{j w_i}$ is the count of word $w_i$ occurs with topic $j$, $q_{j w_i}'$ is the count of word $w_i$ occurs with topic $j$ except for the word $w_i$, $n_{mk}$ is the count of the number of times topic $z_i = k$ in the document $m$ and $n_{mk}'$ is the count of the number of times topic $z_i = k$ in the document $m$ except for the word $w_i$ 

\section{Design and Analysis of Algorithms}

\label{sec:Algorithms}
Mention complexities of algorithms.

\subsection{LDA}

\subsection{Collapsed Gibb's Sampling}
Cite [Heinrich, 2005]

\section{Design of Experiments}
\label{sec:Experiments}

\subsection{Dataset Preprocessing}
Pre-processing was required only for the second data-set. The classic400 dataset was used as provided in the \textsc{Matlab} file format.

The following steps were taken to pre-process the hookah data set : 

\begin{enumerate}
  \item The text is tokenized using the Punkt Tokenizer \cite{punkt} which is part of the Python NLTK library \cite{nltk}. This process splits the tweets into individual words.
  \item Then, the tokens are stemmed using the Porter Stemmer \cite{porter}, also part of the Python NLTK library. This helps map similar words to the same stem. Word variations like plurals and tense-based conjugates are replaced by the root word.
  \item All stopwords were deleted. Stopwords are common words like 'I' ,'me',can' etc. We used a list of 127 stopwords that is provided with NLTK\cite{nltk}.
  \item All words are mapped to unique integer values and represented in the same format as classic400 i.e. a matrix of dimensions K(number of documents) x V(size of vocabulary).
\end{enumerate}

Mention the stopwords used for processing second dataset.
Cite the link : http://cseweb.ucsd.edu/users/elkan/151/classic400.mat

\subsection{Experiments with LDA}
We ran the LDA implementation on both data sets for different number of topics. Specifically, we experimented with K = 3,10,50,100. 

For each experiment, the $\theta$ vector and $\phi$ vector were recorded. From these, we inferred the top 3 topics for the entire data set. The top 3 topics are the 3 topics with which the highest number of words and documents were labeled.

We also tracked the number of topic-word associations that remained the same between consecutive iterations of LDA. This measure served as a indicator of convergence of the distribution. Related plot are presented in the results section.

\subsection{Implementation}
We implement LDA in \textsc{MATLAB}.

The pre-processing code was written in Python. Specifically, the NLTK python library was used for tokenizing, stemming and removing stop words. Details of these procedures are covered in section 4.1.

Some more implementation details that we discovered while developing the code are :

\begin{enumerate}
  \item \red{Z is specific to a document. Therefore it will have a different length for different document.}
  \item \red{Also each word can have a different topic, therefore there will be a different entry for each word in Z. However, this is not true about q and n as they are counts. They will have K * V and K * M sizes.}
\end{enumerate}

\subsection{Code Optimization}
We optimized the following equation in 4 ways :

\begin{equation}
p(z_i = j | \bar{z}', \bar{w}) = \frac{q_{j w_{i}'} + \beta_{w_i}}{\sum_t{q_{jt}' + \beta_t}}\frac{n_{mj}' + \alpha_j}{\sum_k{n_{mk}' + \alpha_k}}
\end{equation}

\begin{enumerate}
\item $q_{j w_{i}'}+\beta_{w_i}$ In this part of the calculation, $\beta_{w_i}$ is constant for any fixed value of j. Therefore, for each j, $\beta_{w_i}$ were added it to all $q_{j w_{i}'}$ outside the loop. 
\item $n_{mj}' + \alpha_j$ For each value of j, $\alpha_j$ is a constant. Therefore, for each j, $\alpha_j$ was added to all $n_{mj}'$ outside the loop.
\item $\sum_t{q_{jt}' + \beta_t}$ For any value of t, $\beta_t$ is a constant value. Therefore, for all t, $\beta_t$ was added to $\sum_t{q_{jt}}'$.
\item $\sum_k{n_{mk}' + \alpha_k}$ For any value of k, $\alpha_k$ is a constant. Therefore, for all k, $\alpha_k$ is added to all $\sum_k{n_{mk}}'$.
\end{enumerate}

\bigskip
One more code optimization saved us a lot of time during experiments. This was in the calculation of the $\phi$ vector. Calculating the $\phi$ vector for all topics is an O(K*V*M) operation. V and M cannot really be optimized. However, we did optimize the K value. We calculated $\phi$ vector for only the top 3 topics. These top 3 topics were the topics with which most documents were associated as per the $\theta$ vector.


\subsection{Sanity Checks}
We used the following checks during the development of the code to ensure that our implementation was correct.
\begin{enumerate}
\item For the classic400 dataset, we had access to the true labels which were grounded in real-world truth. We compared the topic distribution of our learned model to these true labels. 
\item After initializing q and n, the sum of these two vectors should be the same and it should equal the total number of words in the corpus.
\end{enumerate}

\section{Results of Experiments}
\label{sec:Results}

\subsection{Result 1}
Describe results of topic modelling in both data sets.
-Set of words associated with the dominant topics
-3d graph for both data sets 
	-- line graph (showing triangle for first one)
	-- cluster graph
-Some measure of goodness of fit
-Give some results related to kappa, alpha and beta
-Say something about overfitting (if applicable)

Correlate both data sets' result to real-world knowledge of the data.

\paragraph{More details about dataset 1 = classic 400}
\paragraph{More details about dataset 2 = chosen data set}

\subsection{Accuracy of LDA}
Include the plot of theta values.

\section{Findings and Lessons Learned}
\label{Lessons}

\subsection{Goodness of Fit}
ADDRESS THE FOLLOWING

In the report, try to answer the following questions. The questions are related to each other, and do not have definitive answers.
1. What is a sensible way to define the goodness-of-fit, for the same dataset, of LDA models with different hyperparameters K, ALPHA, and BETA? (Refer to tips in class notes)
2. Given the definition of goodness-of-fit, is it possible to compute it numerically, either exactly or approximately?
3. How can we determine whether an LDA model is overfitting its training data?
For the two datasets with which you do experiments, present and justify good values for K, ALPHA and BETA. You can choose these values informally (you do not need an automated algorithm) but your choices should be sensible and justified.

\bibliographystyle{abbrv}
\bibliography{Report}

\end{document}